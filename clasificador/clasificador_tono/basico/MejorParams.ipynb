{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification\n",
    ")\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_with_neutral(text, model, tokenizer, alpha=1.5, neutral_threshold=0.1):\n",
    "    model_device = next(model.parameters()).device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(model_device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1).squeeze().tolist()\n",
    "    neutral_score = 1 - abs(probs[0] - 0.5) ** alpha - abs(probs[2] - 0.5) ** alpha\n",
    "    if neutral_score > (1 - neutral_threshold):\n",
    "        return 1\n",
    "    return int(torch.argmax(torch.tensor(probs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea91acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_excel(\"clasificador_analisis/clasificador/basico/tonos_dataset.xlsx\")\n",
    "df = df.rename(columns={\"Mensaje\": \"text\", \"Etiqueta\": \"label\"})\n",
    "etiqueta_map = {\"Negativo\": 0, \"Neutro\": 1, \"Positivo\": 2}\n",
    "df[\"label\"] = df[\"label\"].map(etiqueta_map)\n",
    "df = df.dropna()\n",
    "\n",
    "model_name = \"VerificadoProfesional/SaBERT-Spanish-Sentiment-Analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_dir = \"clasificador_analisis/clasificador/clasificador_tono/basico/comparativa\"\n",
    "\n",
    "epoch_values = range(2, 6)\n",
    "mejor_accuracy = 0\n",
    "mejor_comb = \"\"\n",
    "mejor_result_path = \"\"\n",
    "mejor_f1_equilibrio = float('inf')\n",
    "cumple_f1_al_menos_una_vez = False\n",
    "\n",
    "accuracy_por_comb = defaultdict(list)\n",
    "recall_dist_por_comb = defaultdict(lambda: {\"neu-neg\": [], \"neu-pos\": [], \"neg-pos\": []})\n",
    "top_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp, df_test = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "df_train, df_valid = train_test_split(df_temp, test_size=0.2, stratify=df_temp[\"label\"], random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "valid_dataset = Dataset.from_pandas(df_valid)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "tokenize = lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "for num_epochs in epoch_values:\n",
    "    comb_key = f\"ep{num_epochs}\"\n",
    "    path = f\"{base_dir}/ep_{num_epochs}\"\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, ignore_mismatched_sizes=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    df_test_eval = test_dataset.to_pandas()\n",
    "    preds = df_test_eval[\"text\"].apply(lambda x: predict_sentiment_with_neutral(x, model, tokenizer))\n",
    "    df_test_eval[\"label_predicted\"] = preds\n",
    "\n",
    "    y_true = df_test_eval[\"label\"]\n",
    "    y_pred = df_test_eval[\"label_predicted\"]\n",
    "\n",
    "    print(\"\\nüìä MATRIZ DE CONFUSI√ìN:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nüìà CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Negativo\", \"Neutro\", \"Positivo\"]))\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, target_names=[\"Negativo\", \"Neutro\", \"Positivo\"])\n",
    "    recall_neg = report[\"Negativo\"][\"recall\"]\n",
    "    recall_neu = report[\"Neutro\"][\"recall\"]\n",
    "    recall_pos = report[\"Positivo\"][\"recall\"]\n",
    "\n",
    "    accuracy_por_comb[comb_key].append(accuracy)\n",
    "    recall_dist_por_comb[comb_key][\"neu-neg\"].append(abs(recall_neu - recall_neg) * 100)\n",
    "    recall_dist_por_comb[comb_key][\"neu-pos\"].append(abs(recall_neu - recall_pos) * 100)\n",
    "    recall_dist_por_comb[comb_key][\"neg-pos\"].append(abs(recall_neg - recall_pos) * 100)\n",
    "\n",
    "    f1s = [f1_score(y_true, y_pred, labels=[i], average=\"macro\") for i in range(3)]\n",
    "    f1_eq = np.std(f1s)\n",
    "\n",
    "    top_results.append({\n",
    "        \"combinacion\": comb_key,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_eq\": f1_eq,\n",
    "        \"ruta\": path\n",
    "    })\n",
    "\n",
    "    es_mejor = (accuracy > mejor_accuracy) or (accuracy == mejor_accuracy and f1_eq < mejor_f1_equilibrio)\n",
    "\n",
    "    if es_mejor:\n",
    "        anterior_path = mejor_result_path\n",
    "        mejor_accuracy = accuracy\n",
    "        mejor_comb = comb_key\n",
    "        mejor_result_path = path\n",
    "        mejor_f1_equilibrio = f1_eq\n",
    "\n",
    "        if anterior_path and anterior_path != path and os.path.exists(anterior_path):\n",
    "            shutil.rmtree(anterior_path)\n",
    "            print(f\"üóëÔ∏è Eliminado modelo anterior: {anterior_path}\")\n",
    "\n",
    "        df_train_export = train_dataset.to_pandas()\n",
    "        df_valid_export = valid_dataset.to_pandas()\n",
    "        df_test_export = test_dataset.to_pandas()\n",
    "\n",
    "        df_train_export.to_excel(\"clasificador_analisis/clasificador/clasificador_tono/basico/mejor_train.xlsx\", index=False)\n",
    "        df_valid_export.to_excel(\"clasificador_analisis/clasificador/clasificador_tono/basico/mejor_valid.xlsx\", index=False)\n",
    "        df_test_export.to_excel(\"clasificador_analisis/clasificador/clasificador_tono/basico/mejor_test.xlsx\", index=False)\n",
    "\n",
    "        model.save_pretrained(f\"{path}/modelo_final\")\n",
    "        tokenizer.save_pretrained(f\"{path}/modelo_final\")\n",
    "\n",
    "        print(f\"üíæ Guardado nuevo mejor modelo: {comb_key} | Accuracy: {accuracy:.4f} | Path: {path}\")\n",
    "    else:\n",
    "        if path != mejor_result_path and os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"‚õî Borrado modelo descartado: {path}\")\n",
    "\n",
    "print(f\"\\nüèÅ Mejor combinaci√≥n final ‚Üí {mejor_comb} | Accuracy: {mejor_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954df2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"clasificador_analisis/clasificador/clasificador_tono/basico/comparativa/ep_4/modelo_final\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"clasificador_analisis/clasificador/clasificador_tono/class_balanced/mejor_test.xlsx\")\n",
    "df = df.rename(columns={\"text\": \"text\", \"label\": \"label_manual_num\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4340be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_with_neutral_alpha_thresh(text, model, tokenizer, alpha, neutral_threshold):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=1).squeeze().tolist()\n",
    "    neutral_score = 1 - abs(probs[0] - 0.5) ** alpha - abs(probs[2] - 0.5) ** alpha\n",
    "    return 1 if neutral_score > (1 - neutral_threshold) else int(torch.argmax(torch.tensor(probs)))\n",
    "\n",
    "results = []\n",
    "alpha_values = np.arange(1.1, 1.3, 0.1)\n",
    "threshold_values = np.arange(0.4, 0.6, 0.1)\n",
    "\n",
    "print(\"\\nüîç Evaluando combinaciones de alpha y threshold...\")\n",
    "for alpha in alpha_values:\n",
    "    for threshold in threshold_values:\n",
    "        preds = df[\"text\"].apply(lambda x: predict_sentiment_with_neutral_alpha_thresh(x, model, tokenizer, alpha, threshold))\n",
    "        f1 = f1_score(df[\"label_manual_num\"], preds, average='macro')\n",
    "        results.append({\"alpha\": round(alpha, 2), \"threshold\": round(threshold, 2), \"f1_macro\": round(f1, 4)})\n",
    "        print(f\"Alpha: {alpha:.1f}, Threshold: {threshold:.1f} ‚Üí F1 Macro: {f1:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "mejor = results_df.iloc[0]\n",
    "print(f\"\\n‚úÖ Mejor combinaci√≥n global ‚Üí Alpha: {mejor['alpha']}, Threshold: {mejor['threshold']}, F1 Macro: {mejor['f1_macro']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
