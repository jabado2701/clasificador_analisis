{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_with_neutral(text, model, tokenizer, alpha=1.5, neutral_threshold=0.1):\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(model(**inputs).logits, dim=1).squeeze()\n",
    "\n",
    "    neutral_score = 1 - (abs(probs[0] - 0.5) ** alpha + abs(probs[2] - 0.5) ** alpha)\n",
    "    return 1 if neutral_score > (1 - neutral_threshold) else int(torch.argmax(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"clasificador_analisis/clasificador/datasets_originales/tonos_dataset.xlsx\")\n",
    "df = df.rename(columns={\"Mensaje\": \"text\", \"Etiqueta\": \"label\"})\n",
    "df[\"label\"] = df[\"label\"].map({\"Negativo\": 0, \"Neutro\": 1, \"Positivo\": 2})\n",
    "df = df.dropna()\n",
    "\n",
    "model_name = \"VerificadoProfesional/SaBERT-Spanish-Sentiment-Analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_dir = \"clasificador_analisis/clasificador/clasificador_tono/classweights_crossentropy/comparativa\"\n",
    "\n",
    "class_weights = 1. / df[\"label\"].value_counts().sort_index()\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "alpha = torch.tensor(class_weights.values, dtype=torch.float)\n",
    "\n",
    "epoch_values = range(4, 5)\n",
    "\n",
    "mejor_accuracy = 0.0\n",
    "mejor_comb = \"\"\n",
    "mejor_f1_equilibrio = float(\"inf\")\n",
    "mejor_result_path = \"\"\n",
    "\n",
    "accuracy_por_comb = defaultdict(list)\n",
    "recall_dist_por_comb = defaultdict(lambda: {\n",
    "    \"neu-neg\": [],\n",
    "    \"neu-pos\": [],\n",
    "    \"neg-pos\": []\n",
    "})\n",
    "top_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "dataset = Dataset.from_pandas(df_balanced).shuffle(seed=42).train_test_split(test_size=0.2)\n",
    "train_valid = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = train_valid[\"train\"]\n",
    "valid_dataset = train_valid[\"test\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "tokenize = lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "for num_epochs in epoch_values:\n",
    "    comb_key = f\"ep{num_epochs}\"\n",
    "    path = f\"{base_dir}/{comb_key}\"\n",
    "    print(f\"\\nüìö Entrenando modelo - √âpocas: {num_epochs}\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, ignore_mismatched_sizes=True)\n",
    "    \n",
    "    def compute_loss(model, inputs, return_outputs=False):\n",
    "        labels = inputs[\"labels\"]\n",
    "        logits = model(**inputs).logits\n",
    "        loss = CrossEntropyLoss(weight=alpha.to(logits.device))(logits, labels)\n",
    "        return (loss, logits) if return_outputs else loss\n",
    "    \n",
    "    model.compute_loss = compute_loss\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    df_test = test_dataset.to_pandas()\n",
    "    df_test[\"label_predicted\"] = df_test[\"text\"].apply(lambda x: predict_sentiment_with_neutral(x, model, tokenizer))\n",
    "\n",
    "    y_true = df_test[\"label\"]\n",
    "    y_pred = df_test[\"label_predicted\"]\n",
    "\n",
    "    print(\"\\nüìä MATRIZ DE CONFUSI√ìN:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print(\"\\nüìà CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Negativo\", \"Neutro\", \"Positivo\"]))\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, target_names=[\"Negativo\", \"Neutro\", \"Positivo\"])\n",
    "    recall_neg, recall_neu, recall_pos = report[\"Negativo\"][\"recall\"], report[\"Neutro\"][\"recall\"], report[\"Positivo\"][\"recall\"]\n",
    "\n",
    "    accuracy_por_comb[comb_key].append(acc)\n",
    "    recall_dist_por_comb[comb_key][\"neu-neg\"].append(abs(recall_neu - recall_neg) * 100)\n",
    "    recall_dist_por_comb[comb_key][\"neu-pos\"].append(abs(recall_neu - recall_pos) * 100)\n",
    "    recall_dist_por_comb[comb_key][\"neg-pos\"].append(abs(recall_neg - recall_pos) * 100)\n",
    "\n",
    "    f1s = [f1_score(y_true, y_pred, labels=[i], average=\"macro\") for i in range(3)]\n",
    "    f1_eq = np.std(f1s)\n",
    "\n",
    "    top_results.append({\n",
    "        \"combinacion\": comb_key,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_eq\": f1_eq,\n",
    "        \"ruta\": path\n",
    "    })\n",
    "\n",
    "    es_mejor = (acc > mejor_accuracy) or (acc == mejor_accuracy and f1_eq < mejor_f1_equilibrio)\n",
    "    if es_mejor:\n",
    "        if mejor_result_path and os.path.exists(mejor_result_path):\n",
    "            shutil.rmtree(mejor_result_path)\n",
    "            print(f\"üóëÔ∏è Eliminado modelo anterior: {mejor_result_path}\")\n",
    "\n",
    "        mejor_accuracy = acc\n",
    "        mejor_comb = comb_key\n",
    "        mejor_result_path = path\n",
    "        mejor_f1_equilibrio = f1_eq\n",
    "\n",
    "        train_dataset.to_pandas().to_excel(f\"clasificador_analisis/clasificador/clasificador_tono/classweights_crossentropy/mejor_train.xlsx\", index=False)\n",
    "        valid_dataset.to_pandas().to_excel(f\"clasificador_analisis/clasificador/clasificador_tono/classweights_crossentropy/mejor_valid.xlsx\", index=False)\n",
    "        test_dataset.to_pandas().to_excel(f\"clasificador_analisis/clasificador/clasificador_tono/classweights_crossentropy/mejor_test.xlsx\", index=False)\n",
    "        model.save_pretrained(f\"{path}/modelo_final\")\n",
    "        tokenizer.save_pretrained(f\"{path}/modelo_final\")\n",
    "\n",
    "        print(f\"üíæ Guardado nuevo mejor modelo: {comb_key} | Accuracy: {acc:.4f} | Path: {path}\")\n",
    "    else:\n",
    "        if path != mejor_result_path and os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"‚õî Borrado modelo descartado: {path}\")\n",
    "\n",
    "print(f\"\\nüèÅ Mejor combinaci√≥n final ‚Üí {mejor_comb} | Accuracy: {mejor_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/clasificador_analisis/clasificador/clasificador_tono/classweights_crossentropy/comparativa/ep_4/modelo_final\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "def predict_sentiment_with_neutral_alpha_thresh(text, model, tokenizer, alpha, neutral_threshold):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(next(model.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1).squeeze()\n",
    "    neutral_score = 1 - (abs(probs[0] - 0.5) ** alpha + abs(probs[2] - 0.5) ** alpha)\n",
    "    if neutral_score > (1 - neutral_threshold):\n",
    "        return 1\n",
    "    else:\n",
    "        return int(torch.argmax(probs))\n",
    "\n",
    "df = pd.read_excel(\"/clasificador_analisis/clasificador/clasificador_tono/classweights_crossentrop/mejor_test.xlsx\")\n",
    "df = df.rename(columns={\"text\": \"text\", \"label\": \"label_manual_num\"})\n",
    "\n",
    "alpha_values = np.arange(1.0, 1.2, 0.1)\n",
    "threshold_values = np.arange(0.1, 0.2, 0.1)\n",
    "\n",
    "best_acc = 0\n",
    "best_combo = None\n",
    "best_preds = None\n",
    "results = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for threshold in threshold_values:\n",
    "        print(f\"üîç Probando Alpha={alpha:.2f}, Threshold={threshold:.2f}\")\n",
    "        preds = df[\"text\"].apply(lambda x: predict_sentiment_with_neutral_alpha_thresh(x, model, tokenizer, alpha, threshold))\n",
    "        acc = accuracy_score(df[\"label_manual_num\"], preds)\n",
    "\n",
    "        results.append({\n",
    "            \"alpha\": round(alpha, 2),\n",
    "            \"threshold\": round(threshold, 2),\n",
    "            \"accuracy\": acc\n",
    "        })\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_combo = (round(alpha, 2), round(threshold, 2))\n",
    "            best_preds = preds\n",
    "\n",
    "            print(f\"\\nüî• Nueva mejor combinaci√≥n encontrada: Alpha={alpha:.2f}, Threshold={threshold:.2f}, Accuracy={acc:.4f}\")\n",
    "            print(\"\\nüìä MATRIZ DE CONFUSI√ìN:\")\n",
    "            print(confusion_matrix(df[\"label_manual_num\"], best_preds))\n",
    "            print(\"\\nüìà CLASSIFICATION REPORT:\")\n",
    "            print(classification_report(\n",
    "                df[\"label_manual_num\"],\n",
    "                best_preds,\n",
    "                target_names=[\"Negativo\", \"Neutro\", \"Positivo\"],\n",
    "                digits=3\n",
    "            ))\n",
    "\n",
    "print(f\"\\nüèÜ Mejor combinaci√≥n final ‚Üí Alpha={best_combo[0]}, Threshold={best_combo[1]}, Accuracy={best_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
